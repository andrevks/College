{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalAnalyser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAlu1QgwmRBV"
      },
      "source": [
        "# Lexical Analyser\n",
        "\n",
        "Owner: André Geraldo \n",
        "\n",
        "Course: Computer Enginnering\n",
        "\n",
        "Steps:\n",
        "\n",
        "1- Read a file containing a source code with a specific syntax\n",
        "\n",
        "2 - User a buffer to take each line and classify them \n",
        "\n",
        "3 - Regex will recognise if a token is valid and store them\n",
        "\n",
        "4 - store each token in the the following format:\n",
        " [Token, Lexema, Line, Column]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtTM2UoAkwsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c2d7cd-d254-478f-b753-ee35ce27bf62"
      },
      "source": [
        "from typing import NamedTuple\n",
        "import re\n",
        "\n",
        "class Token(NamedTuple):\n",
        "  type: str\n",
        "  lexema: str\n",
        "  line: int\n",
        "  col: int\n",
        "\n",
        "class LexicalAnalyser:\n",
        "\n",
        "  def __init__(self, source_code):\n",
        "    self.__keywords = {\n",
        "        'BeginFun','EndFun','if','then','else','elif',\n",
        "        'end','funLoopWhile','do','endFunLoop',\n",
        "        'showMeTheCode','grabInput','funny'\n",
        "        }\n",
        "    self.__token_pair = [\n",
        "        ('TK_NUM', r'\\d+'),\n",
        "        ('TK_ATRIB', r'<-'),\n",
        "        ('TK_PERIOD', r'\\.'),\n",
        "        ('TK_ID', r'[A-Za-z]([A-Za-z]|\\d|_)*'),\n",
        "        ('TK_STRING', r'^\\\"([A-Za-z]|\\d|\\.|%|:| )*\\\"$'), #String com aspas\n",
        "        ('TK_OP_AR', r'[-+*\\/]'), #Operações aritmética\n",
        "        ('TK_OP_RE', r'<>|[=<>]'),#Operação relacional  \n",
        "        ('TK_BOOL', r'[|&]'),       \n",
        "        ('TK_OPEN_P', r'\\('),\n",
        "        ('TK_CLOSE_P', r'\\)'),\n",
        "        ('TK_COMMA', r','),\n",
        "        ('TK_NEW_LINE',r'\\n'),\n",
        "        ('TK_SKIP', r'[\\ \\t\\r]+'),\n",
        "        ('MISMATCH', r'.'),                \n",
        "      ]                            \n",
        "    self.__source_code = source_code\n",
        "\n",
        "  def generateToken(self):\n",
        "    \n",
        "    token_base = self.__token_pair\n",
        "    buffer = self.__source_code\n",
        "    #Cria grupos com o nome de cada par de token no formato: (?P<nomeTk>regex)\n",
        "    #Concatena cada par com '|' que vai servir como OU em passos seguintes\n",
        "    regex_rules = '|'.join('(?P<%s>%s)' % strPair for strPair in token_base)\n",
        "    line = 1\n",
        "    col_start = 0\n",
        "\n",
        "    #tkType: Pega o nome do padrão reconhecido mais recente. Dependendo \n",
        "    #do padrão existem algumas correções antes de salvar como um token\n",
        "    #lexema: pega o valor extraído quando o padrão é reconhecido\n",
        "    #column: calcula o valor inicial da coluno do token encontrado\n",
        "\n",
        "    for matchedPattern in re.finditer(regex_rules, buffer):\n",
        "      tkType = matchedPattern.lastgroup\n",
        "      lexema = matchedPattern.group()\n",
        "      column = matchedPattern.start()+1 - col_start\n",
        "      #print(\"LASTGROUP: \", kind)\n",
        "      #print(\"\\nVALUE: \", mo.group())\n",
        "      #print(\"\\nCOLLUMN: \", mo.start() - line_start)\n",
        "      if tkType == 'TK_NUM':\n",
        "        lexema = int(lexema)\n",
        "      elif lexema in self.__keywords :\n",
        "        tkType = lexema\n",
        "      elif tkType == 'TK_NEW_LINE':\n",
        "        col_start = matchedPattern.end()\n",
        "        line += 1\n",
        "        continue\n",
        "      elif tkType == 'TK_SKIP':\n",
        "        continue\n",
        "      elif tkType == 'MISMATCH':\n",
        "        raise RuntimeError(f'{tkType!r} valor inesperado na linha {line}')\n",
        "      #Retorna uma função geradora a quem chamou. A execução começa\n",
        "      #apenas quando o gerador é iterado\n",
        "      #Vantagem: Nenhuma memória é alocada quando o yield é usado\n",
        "      #O parâmetro é uma tupla nomeada (NamedTuple)\n",
        "      yield Token(tkType,lexema,line, column)\n",
        "    \n",
        "def main():\n",
        "  #read the current fun lang file in test.lang\n",
        "  # and store it in a buffer\n",
        "\n",
        "  content = \"\"\n",
        "  with open(\"lang.txt\", \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "  lexico = LexicalAnalyser(content).generateToken()\n",
        "\n",
        "\n",
        "  for token in lexico:\n",
        "    print(token)\n",
        "    print('\\n')\n",
        "\n",
        "main()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token(type='BeginFun', lexema='BeginFun', line=1, col=1)\n",
            "\n",
            "\n",
            "Token(type='if', lexema='if', line=2, col=2)\n",
            "\n",
            "\n",
            "Token(type='TK_ID', lexema='idade', line=2, col=5)\n",
            "\n",
            "\n",
            "Token(type='TK_OP_RE', lexema='<', line=2, col=11)\n",
            "\n",
            "\n",
            "Token(type='TK_NUM', lexema=10, line=2, col=13)\n",
            "\n",
            "\n",
            "Token(type='TK_BOOL', lexema='&', line=2, col=16)\n",
            "\n",
            "\n",
            "Token(type='TK_ID', lexema='anoNasc', line=2, col=18)\n",
            "\n",
            "\n",
            "Token(type='TK_OP_RE', lexema='<>', line=2, col=26)\n",
            "\n",
            "\n",
            "Token(type='TK_NUM', lexema=10, line=2, col=29)\n",
            "\n",
            "\n",
            "Token(type='then', lexema='then', line=2, col=32)\n",
            "\n",
            "\n",
            "Token(type='funLoopWhile', lexema='funLoopWhile', line=3, col=2)\n",
            "\n",
            "\n",
            "Token(type='TK_ID', lexema='valor_ethereum', line=3, col=15)\n",
            "\n",
            "\n",
            "Token(type='TK_OP_RE', lexema='<', line=3, col=30)\n",
            "\n",
            "\n",
            "Token(type='TK_ID', lexema='valor_bitcoin', line=3, col=32)\n",
            "\n",
            "\n",
            "Token(type='do', lexema='do', line=3, col=46)\n",
            "\n",
            "\n",
            "Token(type='showMeTheCode', lexema='showMeTheCode', line=4, col=3)\n",
            "\n",
            "\n",
            "Token(type='TK_ID', lexema='investYourMoney', line=4, col=17)\n",
            "\n",
            "\n",
            "Token(type='TK_PERIOD', lexema='.', line=4, col=32)\n",
            "\n",
            "\n",
            "Token(type='endFunLoop', lexema='endFunLoop', line=5, col=2)\n",
            "\n",
            "\n",
            "Token(type='TK_PERIOD', lexema='.', line=5, col=12)\n",
            "\n",
            "\n",
            "Token(type='end', lexema='end', line=6, col=2)\n",
            "\n",
            "\n",
            "Token(type='TK_PERIOD', lexema='.', line=6, col=5)\n",
            "\n",
            "\n",
            "Token(type='EndFun', lexema='EndFun', line=7, col=1)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}